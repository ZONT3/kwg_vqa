# KWG VQA

## Цели проекта

- Разработка модели VQA с использованием state-of-the-art технологий
- Тестирование на собственном сгенерированном датасете

## Этот репозиторий

Кодовая база модели VQA

## Авторы

- Код этого репозитория: Вейценфельд Д. А.
- Коллега по проекту: Горбунова Я. М.
- Научный руководитель: Киселев Г. А.

Все вышеперечисленные относятся к:
1. Российский Университет Дружбы Народов,
   Факультет Физико-Математических и Естественных Наук,
   Кафедра Информационных Технологий
2. Институт Системного Анализа ФИЦ ИУ РАН

## Описание системы

Основной модуль - VQA (vqa) содержит подмодули kwg_trainval (далее - trainval) и kwg_dataset.

В подмодуле trainval используется класс Dataset из kwg_dataset для подготовки и получения данных и дальнейшего 
обучения модели на них.

В нем же используется модуль modeling, содержащий конечный KWGModel.

Этот класс - модель VQA, состоящая из модуля текста (KWGText) и кросс-модальности текста со зрением (KWGXModal)

KWGModel обучается и используется для генерирования ответов в trainval.

Данные, передаваемые из Dataset в KWGModel, являются парами вопросов ("сырых", в виде строки) и уже закодированных 
изображений.

Ответ, генерируемый KWGModel является меткой ответа из набора данных (label). Для конвертации из метки в строку 
(ответ на естественном языке) используется метод класса Dataset - label_to_answer(label)

Перед любой из задач (обучение, генерация ответа - train, val (evaluation) ), происходит подготовка набора 
данных - кеширование ответов в метки, а так же кодирование изображений. Вся эта подготовка реализована в 
подмодуле kwg_dataset.

Кодирование изображений происходит в том случае, если обнаружены изображения в наборе данных, которые 
еще не были закодированы (закодированные изображения сохраняются на диск)

Кодирование осуществляется классом FeatureExtractor, который находится в модуле modeling. Этот класс 
использует класс KWGVision - модуль зрения, который отделен от остальных двух (текста и кросс-модальности)

## Текущее состояние и дальнейшее развитие

Работает FeatureExtractor, успешно кодирует любые изображения, используя предобученную Facebook и Microsoft
модель зрения X152-C4. [Oscar](https://github.com/microsoft/Oscar) успешно съедает эти изображения без проблем. 
Оскара самого нет в системе, т.к. он несовместим с текущими версиями зависимостей, 
из-за чего запускается в отдельном окружении conda. В нашей системе все должно работать в рамках одного окружения.

### План

1. Нужно теперь реализовать модули текста и кросс-модальности, используя современную версию 
[Transformers](https://github.com/huggingface/transformers). В оскаре используется очень старая версия, 
поэтому нужно еще раз изучить всю структуру оскара, и сделать такую же модель, как у него, 
но с использованием этой версии transformers.

2. Избавиться от зависимости scene graph benchmark и реализовать модель X152-C4 без каких-либо зависимостей, 
кроме pytorch и подобных базовых вещей.
